
from transformers import MBartTokenizer, MBartForConditionalGeneration

model_name = "IlyaGusev/mbart_ru_sum_gazeta"
tokenizer = MBartTokenizer.from_pretrained(model_name)
model = MBartForConditionalGeneration.from_pretrained(model_name)

# article_text = """
# Мой новый пэт-проект на Go

# Два элемента:
# 1) программа, которая при запуске генерирует рандомные строки, делает запрос на эту строку с припиской .narod.ru, и если возвращается статус 200, то делает запрос на сервер с этим url
# 2) сервер. он будет принимать запросы, валидировать ссылки, и писать в БД

# нужно это всё, чтобы другие люди могли легко мне помогать с парсингом существующих сайтов на данном домене. после примерно 200-500 запросов, мой IP блокировался на некоторое время. я так и не понял, как мне это обойти, поэтому иду на такие меры :)

# уже готов эндпоинт создания и чтения, + сам парсер. не идеально, но уже какой-то результат. в будущем буду дорабатывать и деплоить

# Про опыт разработки на Go
# Пишу на http/net с роутером chi. Это самый минималистичный, тру набор Go-разраба, который обычно является членом "культа стандартной библиотеки". Т.е. он имеет майндсет "все эти ваши фреймворки я на хую вертел, дайте мне простейшие интрументы, чтобы я сам собрал себе всё самое нужное"

# Почему так получилось? Потому что сам Go пропитан минимализмом. Он в некоторой степени анти-джава. Это выражаётся в синтаксисе (максимально простой), в сложности языка (не самый сложный, мало конструкций), в этом вот безфреймворковом подходе, и даже в банальных названиях переменных. Ты тут не увидишь имена типа CanBeDoneableExceptionClassObject, какие мог бы увидеть в Java. Пример:
# conn, var, err, и т.д.

# Почему так? Потому что Go дизайнился как язык для высоких нагрузок, т.е. с низкой задержкой.
 
# И... Да. Я ушёл от темы.

# Теперь точно про опыт разработки на Go

# Не буду писать структурированно, напишу общие наблюдения

# > низкоуровнего, минималистично, надо многое самому собирать
# > быстро запускается и закрывается в докере. компил и скачка библиотек быстрее, чем в питоне
# > на удивление, ошибки мне нравятся. если вылезает ошибка, ты сразу знаешь, где она. у тебя не вылезает 1000 строк эксепшенов. с одной стороны, нету - и хорошо, разработчик не отвлекается. с другой стороны - бля, при работе со сложным проектом, это, наверное, плохо... удачи с дебаггингом
# > код получается ГОРАЗДО более чистым, чем на Python. тот факт, что мне надо обозначать чёткую схему подачи данных в эндпоинт меня уже радует... тут если ты добавил лишний импорт - программа не скомпилируется. в Python я видел десятки лишних импортов, и всем было пох
# """

def summarize_text(article_text: str) -> str:
    input_ids = tokenizer(
        [article_text],
        max_length=600,
        padding="max_length",
        truncation=True,
        return_tensors="pt",
    )["input_ids"]

    output_ids = model.generate(
        input_ids=input_ids,
        no_repeat_ngram_size=4
    )[0]

    summary = tokenizer.decode(output_ids, skip_special_tokens=True)
    return summary


# print result: Старик Иван живёт размеренной жизнью в деревне, выращивая овощи и находя радость в простых вещах.
